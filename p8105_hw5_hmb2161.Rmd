---
title: "hw5"
output: github_document
---

```{r setup, include=FALSE}

# Set Up
library(tidyverse)
library(data.table)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_bw() + theme(legend.position = "bottom"))
```

# Problem 1
```{r}
# Code Jeff gave us
set.seed(10)

iris_with_missing = iris %>% 
  map_df(~replace(.x, sample(1:150, 20), NA)) %>%
  mutate(Species = as.character(Species))

is.list(iris_with_missing)

# creating a data frame for if the value is numeric, taking the mean of the column x and save it as a value y, replacing any missing values in column x with the value y, assessing if the value is character variable, and replacing missing values in column x with virginica

missing_value = function(x) {
  if (is.numeric(x)) {                  
    y = mean(x, na.rm = TRUE)             
    x = replace(x, is.na(x), y)           
  } else if (is.character(x)) {          
    x = replace(x, is.na(x), "virginica") 
  }
}

# create an output where i map my function onto each variable in my dataframe
output = map_df(iris_with_missing, missing_value)   

# checking to see if my output really works
output 

```
Each missing numeric value was replaced by the mean value of that column and for the species variable each missing value was replaced with "virginica". 

# Problem 2: data

```{r}

# Creating a tibble of data with file names and data, creating a variable "files" for filenames, creating a variable "data" where i read the csv files, and finally unnesting the variable "data"

prob_data = tibble(
  subject = list.files(path = "./hw5_data",      
                     pattern = ".csv$", 
                     full.names = TRUE)) %>%
  mutate(data =  map(subject, read_csv)) %>%    
  unnest()                                   

#tidy the resulting dataset: pivoting longer to make each column a variable, extracting out group, recoding group variable, and selecting in the order I want 

prob_data =
pivot_longer(prob_data, 
             week_1:week_8,         
             names_to = "week",
             values_to = "observations") %>%
  mutate(subject = str_extract(subject, "(con|exp)_\\d{2}"), 
         group = str_extract(subject, "con|exp"),
         group = fct_recode(group, "control" = "con",    
                            "experiment" = "exp")) %>%
  select(subject, group, week, observations)       

# making a nice table

knitr::kable(prob_data)  
```
I pulled in each file and the data inside each file. I then turned this data into a tidy dataset with each subject in a row. I left con and exp in the subject id as they uniquely identify participants as participant 1 in the control group or experimental group. Ideally I would have created an autonumber id for these participants when inputting the data into the csv's. 

# Problem 2: plot

```{r}

# creating a spaghetti plot for the dataframe created above with week on the x axis, observations on the y axis, grouped by subject, with the color as the treatment arm

plot_ppt_data = 
  ggplot(prob_data, aes(x = week, y = observations, color = group, group = subject)) +
  geom_point(aes(color = subject)) +
  geom_line() +
  viridis::scale_color_viridis(aes(color = group), discrete = TRUE) +
  labs(
    title = "Participant Observations Over Weeks By Treatment Group",
    x = "Weeks",
    y = "Observation Values"
  )

plot_ppt_data
```

The resulting plot shows the observation values for each participant over the course of 8 weeks. The control group appear to have lower observation values across weeks of the study compared to the experimental arm. The participants generally start from a similar baseline (the groups are overlapped), but as the trial moves forward, the measurements get much higher for the experimental group and stay about the same for the control group. 

# Problem 3

```{r}
# creating an equation simulation for n = 30, beta0 = 2, and beta1 = 0

sim_regression = function(n = 30, beta0 = 2, beta1 = 0) {
  sim_data = tibble(
    x = rnorm(n = 30, mean = 1, sd = 1),
    y = beta0 + beta1 * x + rnorm(n, 0, 50)    
  )
  
  ls_fit = lm(y ~ x, data = sim_data) %>%
    broom::tidy() %>%
    select(term, estimate, p.value)

}

# creating a dataframe with the regression simulation run 10,000 times for beta1 = {0, 1, 2, 3, 4, 5, 6} including p-values and estimated beta1 value

models = 
  tibble(beta1 = 0:6) %>%
  mutate(reg_data = map(.x = beta1, ~rerun(10000, sim_regression(beta1 = .x))),
         estimate_dfs = map(reg_data, bind_rows)) %>%
  select(-reg_data) %>%
  unnest(estimate_dfs)

# showing an example few rows of my simulation results

knitr::kable(head(models))

```
The regression simulation ran a regression for random values that are normally distributed with n = 30, mean = 1, and sd = 1. When I created my tibble I reran the simulation 10,000 times and saved each beta 1, estimated beta 1, and p-value. 

## Problem 3: Null Rejection by Beta1

```{r}

# creating a dataframe grouped by beta1 and summarized by the p-value > 0.05 with a proportion of times I rejected the null

models_reject = 
  models %>%
  group_by(beta1) %>%
  count(reject = p.value < 0.05) %>%
  mutate(reject_pct = n/sum(n) * 100) %>%
  filter(reject == TRUE)
  
# creating graph of when we rejected the null against the beta1 value 

rejected_null = 
ggplot(models_reject, aes(x = beta1, 
                          y = reject_pct, 
                          color = reject_pct)) +
  geom_point() +
  geom_line() +
  viridis::scale_color_viridis(aes(color = reject_pct), 
                               discrete = FALSE) +
  labs(title = "Percent of Null Rejection by Beta 1",
       x = "Beta 1",
       y = "Percent of Rejected Null" )

rejected_null
    
```
This plot shows the dataset above filtered for when the p-value is less than 0.05. We reject the p-value when it is below this alpha of 0.05. The graph displays a positive linear association between beta 1 and percent of rejected null. This indicates that as the effect size (beta1) gets larger, the percent of times the null is rejected increases, so the power increases.

## Problem 3: Average Beta1 vs true Beta1

```{r}
# taking the models dataset, grouping by beta1, and taking the average of the beta1

models_avg = 
  models %>%
  group_by(beta1) %>%
  summarize(avg_beta1 = mean(estimate))

# graphing the average beta1 by color

average_beta1 = 
  ggplot(models_avg, aes(x = beta1, y = avg_beta1, color = beta1)) +
  geom_point() + 
  geom_line() +
    viridis::scale_color_viridis(aes(color = beta1), 
                               discrete = FALSE) +
  labs(title = "Avg Beta1 Compared to True Beta1",
       x = "True Beta 1",
       y = "Average Beta 1" )

average_beta1

```
I averaged the "estimate" value in my initial tibble to create an average beta 1 variable and graphed it against the true beta1. The average beta 1 and true beta 1 have a positive linear association. Average beta 1 trends lower than the true beta 1.

## Problem 3: Average Beta vs True Beta Among Rejected Nulls
```{r}
# created dataframe with average betas by p-values less than 0.05
avg_reject = 
  models %>%
  filter(p.value < 0.05) %>%
  group_by(beta1) %>%
  summarize(avg_beta1 = mean(estimate))
  
avg_reject

# plotted the dataframe above by true beta1 vs average beta1 for those values that were less than 0.05

plot_two = 
  ggplot(avg_reject, aes(x = beta1, y = avg_beta1, color = avg_beta1)) +
  geom_point() +
  geom_line() +
  viridis::scale_color_viridis(aes(color = reject_pct), 
                               discrete = FALSE) +
  labs(title = "Avg Beta1 Compared to True Beta1 Among Rejected P.Values",
       x = "True Beta 1",
       y = "Average Beta 1" )

plot_two
```
No, the sample average of the average beta 1 is not equal to the true value of beta 1 across reject null tests. This is because the p-values that get rejected are more likely to have a larger effect size, so they will have a larger average beta 1 in comparison to the true beta 1. Across both of the plots of average and true beta 1 there is a positive linear correlation, but in the plot for rejected nulls, the effect size is larger so on average the beta 1 will be larger. 
